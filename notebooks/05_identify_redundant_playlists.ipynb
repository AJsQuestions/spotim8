{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# ðŸ—‘ï¸ Identify Redundant Playlists & Reorganization\n",
        "\n",
        "Analyze your playlists to identify **redundant playlists** that can be safely deleted or merged without losing information.\n",
        "\n",
        "**What this notebook does:**\n",
        "- ðŸ” Finds playlists with high track overlap (duplicates/near-duplicates)\n",
        "- ðŸ“Š Identifies playlists that are subsets of other playlists\n",
        "- ðŸŽ¯ Suggests playlists safe to delete\n",
        "- ðŸ“‹ Proposes a reorganized library structure\n",
        "- ðŸ’¡ Recommends consolidation strategies\n",
        "\n",
        "**Prerequisites:** Run `01_sync_data.ipynb` first to download your library.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1ï¸âƒ£ Setup\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install dependencies\n",
        "%pip install -q pandas pyarrow tqdm\n",
        "\n",
        "# Add project to path\n",
        "import sys\n",
        "from pathlib import Path\n",
        "PROJECT_ROOT = Path(\"..\").resolve()\n",
        "if str(PROJECT_ROOT) not in sys.path:\n",
        "    sys.path.insert(0, str(PROJECT_ROOT))\n",
        "\n",
        "print(f\"âœ… Project root: {PROJECT_ROOT}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from pathlib import Path\n",
        "from collections import defaultdict, Counter\n",
        "from typing import Dict, List, Set, Tuple\n",
        "from tqdm.auto import tqdm\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "from spotim8.analysis import LibraryAnalyzer, PlaylistSimilarityEngine\n",
        "\n",
        "# Data directory\n",
        "DATA_DIR = PROJECT_ROOT / \"data\"\n",
        "print(f\"ðŸ“ Data directory: {DATA_DIR.resolve()}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2ï¸âƒ£ Load Library Data\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load all data (owned playlists only for analysis)\n",
        "analyzer = LibraryAnalyzer(DATA_DIR).load()\n",
        "\n",
        "# Get owned playlists\n",
        "playlists = analyzer.playlists_all[analyzer.playlists_all['is_owned'] == True].copy()\n",
        "playlist_tracks = analyzer.playlist_tracks_all[\n",
        "    analyzer.playlist_tracks_all['playlist_id'].isin(playlists['playlist_id'])\n",
        "].copy()\n",
        "\n",
        "print(f\"âœ… Loaded {len(playlists)} owned playlists\")\n",
        "print(f\"ðŸ“Š Total playlist-track links: {len(playlist_tracks):,}\")\n",
        "\n",
        "# Exclude Liked Songs from redundancy analysis (it's the master playlist)\n",
        "liked_id = analyzer.liked_songs_id\n",
        "if liked_id:\n",
        "    playlists = playlists[playlists['playlist_id'] != liked_id].copy()\n",
        "    print(f\"   (Excluded Liked Songs from analysis)\")\n",
        "\n",
        "print(f\"\\nðŸ“‹ Analyzing {len(playlists)} playlists for redundancy...\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3ï¸âƒ£ Build Playlist Track Sets\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Build track sets for each playlist\n",
        "playlist_track_sets: Dict[str, Set[str]] = {}\n",
        "playlist_info = {}\n",
        "\n",
        "for pid in tqdm(playlists['playlist_id'], desc=\"Building track sets\"):\n",
        "    tracks = set(playlist_tracks[playlist_tracks['playlist_id'] == pid]['track_id'].unique())\n",
        "    playlist_track_sets[pid] = tracks\n",
        "    \n",
        "    info = playlists[playlists['playlist_id'] == pid].iloc[0]\n",
        "    playlist_info[pid] = {\n",
        "        'name': info.get('name', 'Unknown'),\n",
        "        'track_count': len(tracks),\n",
        "        'is_liked_songs': info.get('is_liked_songs', False),\n",
        "    }\n",
        "\n",
        "print(f\"âœ… Built track sets for {len(playlist_track_sets)} playlists\")\n",
        "print(f\"ðŸ“Š Total unique tracks across all playlists: {len(set().union(*playlist_track_sets.values())):,}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4ï¸âƒ£ Find Redundant Playlists\n",
        "\n",
        "We'll identify redundancy using multiple criteria:\n",
        "1. **Exact duplicates** - Same tracks\n",
        "2. **Subsets** - All tracks in one playlist are in another\n",
        "3. **High overlap** - Very similar track sets (>90% overlap)\n",
        "4. **Near-duplicates** - High similarity with minor differences\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def jaccard_similarity(set1: Set, set2: Set) -> float:\n",
        "    \"\"\"Calculate Jaccard similarity (intersection / union).\"\"\"\n",
        "    if not set1 and not set2:\n",
        "        return 1.0\n",
        "    if not set1 or not set2:\n",
        "        return 0.0\n",
        "    intersection = len(set1 & set2)\n",
        "    union = len(set1 | set2)\n",
        "    return intersection / union if union > 0 else 0.0\n",
        "\n",
        "def overlap_ratio(set1: Set, set2: Set) -> Tuple[float, float]:\n",
        "    \"\"\"Calculate overlap ratios in both directions.\n",
        "    \n",
        "    Returns:\n",
        "        (ratio of set1 in set2, ratio of set2 in set1)\n",
        "    \"\"\"\n",
        "    if not set1 or not set2:\n",
        "        return (0.0, 0.0)\n",
        "    intersection = len(set1 & set2)\n",
        "    return (intersection / len(set1), intersection / len(set2))\n",
        "\n",
        "# Find redundant playlists\n",
        "redundant_groups = []\n",
        "exact_duplicates = []\n",
        "subsets = []  # (subset_playlist, superset_playlist)\n",
        "high_overlap = []  # (>90% similarity)\n",
        "near_duplicates = []  # (80-90% similarity)\n",
        "\n",
        "playlist_ids = list(playlist_track_sets.keys())\n",
        "\n",
        "print(\"ðŸ” Analyzing playlist pairs...\")\n",
        "for i in tqdm(range(len(playlist_ids)), desc=\"Comparing playlists\"):\n",
        "    pid1 = playlist_ids[i]\n",
        "    set1 = playlist_track_sets[pid1]\n",
        "    \n",
        "    if not set1:  # Skip empty playlists\n",
        "        continue\n",
        "    \n",
        "    for j in range(i + 1, len(playlist_ids)):\n",
        "        pid2 = playlist_ids[j]\n",
        "        set2 = playlist_track_sets[pid2]\n",
        "        \n",
        "        if not set2:  # Skip empty playlists\n",
        "            continue\n",
        "        \n",
        "        # Check for exact duplicates\n",
        "        if set1 == set2:\n",
        "            exact_duplicates.append((pid1, pid2))\n",
        "            continue\n",
        "        \n",
        "        # Check for subsets\n",
        "        if set1.issubset(set2):\n",
        "            subsets.append((pid1, pid2, len(set1), len(set2)))\n",
        "        elif set2.issubset(set1):\n",
        "            subsets.append((pid2, pid1, len(set2), len(set1)))\n",
        "        \n",
        "        # Calculate similarity\n",
        "        jaccard = jaccard_similarity(set1, set2)\n",
        "        overlap1, overlap2 = overlap_ratio(set1, set2)\n",
        "        \n",
        "        if jaccard > 0.9:\n",
        "            high_overlap.append((pid1, pid2, jaccard, overlap1, overlap2))\n",
        "        elif jaccard > 0.8:\n",
        "            near_duplicates.append((pid1, pid2, jaccard, overlap1, overlap2))\n",
        "\n",
        "print(f\"\\nâœ… Analysis complete!\")\n",
        "print(f\"   Exact duplicates: {len(exact_duplicates)}\")\n",
        "print(f\"   Subsets: {len(subsets)}\")\n",
        "print(f\"   High overlap (>90%): {len(high_overlap)}\")\n",
        "print(f\"   Near duplicates (80-90%): {len(near_duplicates)}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Display exact duplicates\n",
        "if exact_duplicates:\n",
        "    print(\"=\" * 80)\n",
        "    print(\"ðŸ”„ EXACT DUPLICATES (Same tracks, can delete one)\")\n",
        "    print(\"=\" * 80)\n",
        "    for pid1, pid2 in exact_duplicates:\n",
        "        info1 = playlist_info[pid1]\n",
        "        info2 = playlist_info[pid2]\n",
        "        print(f\"\\nðŸ“‹ {info1['name']} ({info1['track_count']} tracks)\")\n",
        "        print(f\"   âš¡ Duplicate of: {info2['name']} ({info2['track_count']} tracks)\")\n",
        "        print(f\"   ðŸ’¡ Recommendation: Delete one (keep the one with better name)\")\n",
        "else:\n",
        "    print(\"âœ… No exact duplicates found\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Display subsets (playlists fully contained in others)\n",
        "if subsets:\n",
        "    print(\"\\n\" + \"=\" * 80)\n",
        "    print(\"ðŸ“¦ SUBSETS (Fully contained in another playlist - SAFE TO DELETE)\")\n",
        "    print(\"=\" * 80)\n",
        "    \n",
        "    subset_df = []\n",
        "    for subset_pid, superset_pid, subset_size, superset_size in subsets:\n",
        "        subset_info = playlist_info[subset_pid]\n",
        "        superset_info = playlist_info[superset_pid]\n",
        "        subset_df.append({\n",
        "            'Subset Playlist': subset_info['name'],\n",
        "            'Subset Tracks': subset_size,\n",
        "            'Contained In': superset_info['name'],\n",
        "            'Superset Tracks': superset_size,\n",
        "            'Coverage': f\"{subset_size}/{superset_size} ({100*subset_size/superset_size:.1f}%)\"\n",
        "        })\n",
        "    \n",
        "    df = pd.DataFrame(subset_df)\n",
        "    df = df.sort_values('Subset Tracks', ascending=False)\n",
        "    print(f\"\\nðŸ“Š Found {len(df)} playlists that are subsets of others:\\n\")\n",
        "    print(df.to_string(index=False))\n",
        "    \n",
        "    # Identify unique subset playlists (safe to delete)\n",
        "    safe_to_delete = set([subset_pid for subset_pid, _, _, _ in subsets])\n",
        "    print(f\"\\nâœ… {len(safe_to_delete)} playlists can be safely deleted (all tracks are in other playlists)\")\n",
        "else:\n",
        "    print(\"\\nâœ… No subset playlists found\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Display high overlap playlists\n",
        "if high_overlap:\n",
        "    print(\"\\n\" + \"=\" * 80)\n",
        "    print(\"ðŸŽ¯ HIGH OVERLAP (>90% similarity - Likely redundant)\")\n",
        "    print(\"=\" * 80)\n",
        "    \n",
        "    overlap_df = []\n",
        "    for pid1, pid2, jaccard, overlap1, overlap2 in high_overlap:\n",
        "        info1 = playlist_info[pid1]\n",
        "        info2 = playlist_info[pid2]\n",
        "        overlap_df.append({\n",
        "            'Playlist 1': info1['name'],\n",
        "            'Tracks 1': info1['track_count'],\n",
        "            'Playlist 2': info2['name'],\n",
        "            'Tracks 2': info2['track_count'],\n",
        "            'Similarity': f\"{jaccard*100:.1f}%\",\n",
        "            'P1 in P2': f\"{overlap1*100:.1f}%\",\n",
        "            'P2 in P1': f\"{overlap2*100:.1f}%\",\n",
        "        })\n",
        "    \n",
        "    df = pd.DataFrame(overlap_df)\n",
        "    df = df.sort_values('Similarity', ascending=False)\n",
        "    print(f\"\\nðŸ“Š Found {len(df)} playlist pairs with >90% similarity:\\n\")\n",
        "    print(df.to_string(index=False))\n",
        "    \n",
        "    print(\"\\nðŸ’¡ Recommendations:\")\n",
        "    print(\"   - If one playlist is much smaller, consider merging into the larger one\")\n",
        "    print(\"   - If playlists serve different purposes, keep both but remove duplicate tracks\")\n",
        "else:\n",
        "    print(\"\\nâœ… No high-overlap playlists found\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Display near-duplicates\n",
        "if near_duplicates:\n",
        "    print(\"\\n\" + \"=\" * 80)\n",
        "    print(\"ðŸ”— NEAR-DUPLICATES (80-90% similarity - Review for consolidation)\")\n",
        "    print(\"=\" * 80)\n",
        "    \n",
        "    near_df = []\n",
        "    for pid1, pid2, jaccard, overlap1, overlap2 in near_duplicates[:20]:  # Show top 20\n",
        "        info1 = playlist_info[pid1]\n",
        "        info2 = playlist_info[pid2]\n",
        "        near_df.append({\n",
        "            'Playlist 1': info1['name'],\n",
        "            'Tracks 1': info1['track_count'],\n",
        "            'Playlist 2': info2['name'],\n",
        "            'Tracks 2': info2['track_count'],\n",
        "            'Similarity': f\"{jaccard*100:.1f}%\",\n",
        "            'P1 in P2': f\"{overlap1*100:.1f}%\",\n",
        "            'P2 in P1': f\"{overlap2*100:.1f}%\",\n",
        "        })\n",
        "    \n",
        "    df = pd.DataFrame(near_df)\n",
        "    df = df.sort_values('Similarity', ascending=False)\n",
        "    print(f\"\\nðŸ“Š Top {len(df)} near-duplicate pairs (showing first 20):\\n\")\n",
        "    print(df.to_string(index=False))\n",
        "    \n",
        "    if len(near_duplicates) > 20:\n",
        "        print(f\"\\n   ... and {len(near_duplicates) - 20} more pairs\")\n",
        "    \n",
        "    print(\"\\nðŸ’¡ Recommendations:\")\n",
        "    print(\"   - Review these pairs manually\")\n",
        "    print(\"   - Consider merging if they serve the same purpose\")\n",
        "    print(\"   - Keep separate if they have distinct purposes\")\n",
        "else:\n",
        "    print(\"\\nâœ… No near-duplicate playlists found\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6ï¸âƒ£ Comprehensive Redundancy Analysis\n",
        "\n",
        "Now let's identify ALL playlists that can be safely deleted or consolidated.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Build comprehensive list of playlists safe to delete\n",
        "safe_to_delete = set()\n",
        "consolidation_suggestions = []\n",
        "\n",
        "# 1. Exact duplicates - keep the first one, delete others\n",
        "for pid1, pid2 in exact_duplicates:\n",
        "    # Keep the one with more descriptive name or more tracks\n",
        "    info1 = playlist_info[pid1]\n",
        "    info2 = playlist_info[pid2]\n",
        "    if len(info1['name']) >= len(info2['name']):\n",
        "        safe_to_delete.add(pid2)\n",
        "        consolidation_suggestions.append({\n",
        "            'action': 'delete',\n",
        "            'playlist_id': pid2,\n",
        "            'playlist_name': info2['name'],\n",
        "            'reason': f'Exact duplicate of \"{info1[\"name\"]}\"',\n",
        "            'tracks_lost': 0,\n",
        "            'alternative': info1['name']\n",
        "        })\n",
        "    else:\n",
        "        safe_to_delete.add(pid1)\n",
        "        consolidation_suggestions.append({\n",
        "            'action': 'delete',\n",
        "            'playlist_id': pid1,\n",
        "            'playlist_name': info1['name'],\n",
        "            'reason': f'Exact duplicate of \"{info2[\"name\"]}\"',\n",
        "            'tracks_lost': 0,\n",
        "            'alternative': info2['name']\n",
        "        })\n",
        "\n",
        "# 2. Subsets - safe to delete (all tracks are in superset)\n",
        "for subset_pid, superset_pid, subset_size, superset_size in subsets:\n",
        "    if subset_pid not in safe_to_delete:\n",
        "        subset_info = playlist_info[subset_pid]\n",
        "        superset_info = playlist_info[superset_pid]\n",
        "        safe_to_delete.add(subset_pid)\n",
        "        consolidation_suggestions.append({\n",
        "            'action': 'delete',\n",
        "            'playlist_id': subset_pid,\n",
        "            'playlist_name': subset_info['name'],\n",
        "            'reason': f'All {subset_size} tracks are in \"{superset_info[\"name\"]}\" ({superset_size} tracks)',\n",
        "            'tracks_lost': 0,\n",
        "            'alternative': superset_info['name']\n",
        "        })\n",
        "\n",
        "# 3. High overlap - suggest merging\n",
        "for pid1, pid2, jaccard, overlap1, overlap2 in high_overlap:\n",
        "    info1 = playlist_info[pid1]\n",
        "    info2 = playlist_info[pid2]\n",
        "    \n",
        "    # Determine which to keep (prefer larger or better name)\n",
        "    if info1['track_count'] > info2['track_count']:\n",
        "        keep_pid, delete_pid = pid1, pid2\n",
        "        keep_info, delete_info = info1, info2\n",
        "        missing_tracks = len(playlist_track_sets[pid2] - playlist_track_sets[pid1])\n",
        "    elif info2['track_count'] > info1['track_count']:\n",
        "        keep_pid, delete_pid = pid2, pid1\n",
        "        keep_info, delete_info = info2, info1\n",
        "        missing_tracks = len(playlist_track_sets[pid1] - playlist_track_sets[pid2])\n",
        "    else:\n",
        "        # Same size, keep the one with longer name (usually more descriptive)\n",
        "        if len(info1['name']) >= len(info2['name']):\n",
        "            keep_pid, delete_pid = pid1, pid2\n",
        "            keep_info, delete_info = info1, info2\n",
        "            missing_tracks = len(playlist_track_sets[pid2] - playlist_track_sets[pid1])\n",
        "        else:\n",
        "            keep_pid, delete_pid = pid2, pid1\n",
        "            keep_info, delete_info = info2, info1\n",
        "            missing_tracks = len(playlist_track_sets[pid1] - playlist_track_sets[pid2])\n",
        "    \n",
        "    if delete_pid not in safe_to_delete:\n",
        "        safe_to_delete.add(delete_pid)\n",
        "        consolidation_suggestions.append({\n",
        "            'action': 'merge',\n",
        "            'playlist_id': delete_pid,\n",
        "            'playlist_name': delete_info['name'],\n",
        "            'reason': f'{jaccard*100:.1f}% similar to \"{keep_info[\"name\"]}\"',\n",
        "            'tracks_lost': missing_tracks,\n",
        "            'alternative': f'Merge into \"{keep_info[\"name\"]}\" (add {missing_tracks} missing tracks)'\n",
        "        })\n",
        "\n",
        "print(f\"âœ… Identified {len(safe_to_delete)} playlists that can be safely deleted/merged\")\n",
        "print(f\"ðŸ“Š Total consolidation suggestions: {len(consolidation_suggestions)}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Display comprehensive deletion/consolidation recommendations\n",
        "if consolidation_suggestions:\n",
        "    print(\"\\n\" + \"=\" * 80)\n",
        "    print(\"ðŸ“‹ CONSOLIDATION RECOMMENDATIONS\")\n",
        "    print(\"=\" * 80)\n",
        "    \n",
        "    df = pd.DataFrame(consolidation_suggestions)\n",
        "    df = df.sort_values(['tracks_lost', 'playlist_name'])\n",
        "    \n",
        "    # Separate by action type\n",
        "    delete_actions = df[df['action'] == 'delete']\n",
        "    merge_actions = df[df['action'] == 'merge']\n",
        "    \n",
        "    if len(delete_actions) > 0:\n",
        "        print(f\"\\nðŸ—‘ï¸  SAFE TO DELETE ({len(delete_actions)} playlists - 0 tracks lost):\")\n",
        "        print(\"-\" * 80)\n",
        "        for _, row in delete_actions.iterrows():\n",
        "            print(f\"   â€¢ {row['playlist_name']}\")\n",
        "            print(f\"     â†’ {row['reason']}\")\n",
        "            print(f\"     â†’ Keep: {row['alternative']}\")\n",
        "            print()\n",
        "    \n",
        "    if len(merge_actions) > 0:\n",
        "        print(f\"\\nðŸ”€ MERGE RECOMMENDATIONS ({len(merge_actions)} playlists):\")\n",
        "        print(\"-\" * 80)\n",
        "        for _, row in merge_actions.iterrows():\n",
        "            print(f\"   â€¢ {row['playlist_name']}\")\n",
        "            print(f\"     â†’ {row['reason']}\")\n",
        "            if row['tracks_lost'] > 0:\n",
        "                print(f\"     â†’ âš ï¸  {row['tracks_lost']} unique tracks would need to be added to {row['alternative']}\")\n",
        "            else:\n",
        "                print(f\"     â†’ âœ… No tracks lost - safe to merge\")\n",
        "            print()\n",
        "    \n",
        "    # Summary statistics\n",
        "    total_tracks_lost = df['tracks_lost'].sum()\n",
        "    zero_loss = len(df[df['tracks_lost'] == 0])\n",
        "    \n",
        "    print(\"\\n\" + \"=\" * 80)\n",
        "    print(\"ðŸ“Š SUMMARY\")\n",
        "    print(\"=\" * 80)\n",
        "    print(f\"   Total playlists recommended for deletion/merge: {len(df)}\")\n",
        "    print(f\"   Playlists with zero track loss: {zero_loss}\")\n",
        "    print(f\"   Total unique tracks that would need to be added: {total_tracks_lost}\")\n",
        "    print(f\"   Current total playlists: {len(playlists)}\")\n",
        "    print(f\"   After consolidation: {len(playlists) - len(df)} playlists\")\n",
        "    print(f\"   Reduction: {len(df)} playlists ({100*len(df)/len(playlists):.1f}%)\")\n",
        "else:\n",
        "    print(\"\\nâœ… No consolidation recommendations - your library is well organized!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7ï¸âƒ£ Detailed Track-Level Analysis\n",
        "\n",
        "For merge recommendations, let's see exactly which tracks would need to be added.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# For merge actions, show which tracks need to be added\n",
        "merge_details = []\n",
        "\n",
        "for suggestion in consolidation_suggestions:\n",
        "    if suggestion['action'] == 'merge' and suggestion['tracks_lost'] > 0:\n",
        "        delete_pid = suggestion['playlist_id']\n",
        "        delete_tracks = playlist_track_sets[delete_pid]\n",
        "        \n",
        "        # Find the playlist to merge into\n",
        "        keep_name = suggestion['alternative'].split('\"')[1] if '\"' in suggestion['alternative'] else suggestion['alternative']\n",
        "        keep_pid = None\n",
        "        for pid, info in playlist_info.items():\n",
        "            if info['name'] == keep_name:\n",
        "                keep_pid = pid\n",
        "                break\n",
        "        \n",
        "        if keep_pid:\n",
        "            keep_tracks = playlist_track_sets[keep_pid]\n",
        "            missing_tracks = delete_tracks - keep_tracks\n",
        "            \n",
        "            if missing_tracks:\n",
        "                # Get track names\n",
        "                missing_track_ids = list(missing_tracks)[:10]  # Show first 10\n",
        "                tracks_df = analyzer.tracks_all[analyzer.tracks_all['track_id'].isin(missing_track_ids)]\n",
        "                track_names = tracks_df[['name']].values.flatten().tolist() if len(tracks_df) > 0 else []\n",
        "                \n",
        "                merge_details.append({\n",
        "                    'Delete': suggestion['playlist_name'],\n",
        "                    'Merge Into': keep_name,\n",
        "                    'Missing Tracks': len(missing_tracks),\n",
        "                    'Sample Tracks': ', '.join(track_names[:5]) if track_names else 'N/A'\n",
        "                })\n",
        "\n",
        "if merge_details:\n",
        "    print(\"ðŸ“‹ Detailed Merge Analysis (tracks that need to be added):\")\n",
        "    print(\"=\" * 80)\n",
        "    df = pd.DataFrame(merge_details)\n",
        "    for _, row in df.iterrows():\n",
        "        print(f\"\\nðŸ—‘ï¸  Delete: {row['Delete']}\")\n",
        "        print(f\"   â†’ Merge into: {row['Merge Into']}\")\n",
        "        print(f\"   â†’ Add {row['Missing Tracks']} tracks\")\n",
        "        if row['Sample Tracks'] != 'N/A':\n",
        "            print(f\"   â†’ Sample: {row['Sample Tracks']}...\")\n",
        "else:\n",
        "    print(\"âœ… All merge recommendations have zero track loss!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8ï¸âƒ£ Reorganization Strategy\n",
        "\n",
        "Based on the analysis, here's a suggested reorganization of your library.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Build reorganization plan\n",
        "reorganization_plan = {\n",
        "    'delete': [],\n",
        "    'merge': [],\n",
        "    'keep': []\n",
        "}\n",
        "\n",
        "# Categorize all playlists\n",
        "all_playlist_ids = set(playlist_track_sets.keys())\n",
        "to_delete_ids = safe_to_delete\n",
        "to_keep_ids = all_playlist_ids - to_delete_ids\n",
        "\n",
        "# Build merge groups\n",
        "merge_groups = defaultdict(list)\n",
        "for suggestion in consolidation_suggestions:\n",
        "    if suggestion['action'] == 'merge':\n",
        "        keep_name = suggestion['alternative'].split('\"')[1] if '\"' in suggestion['alternative'] else suggestion['alternative']\n",
        "        merge_groups[keep_name].append(suggestion['playlist_name'])\n",
        "\n",
        "# Organize recommendations\n",
        "for suggestion in consolidation_suggestions:\n",
        "    if suggestion['action'] == 'delete':\n",
        "        reorganization_plan['delete'].append({\n",
        "            'name': suggestion['playlist_name'],\n",
        "            'reason': suggestion['reason'],\n",
        "            'alternative': suggestion['alternative']\n",
        "        })\n",
        "    elif suggestion['action'] == 'merge':\n",
        "        reorganization_plan['merge'].append({\n",
        "            'name': suggestion['playlist_name'],\n",
        "            'reason': suggestion['reason'],\n",
        "            'merge_into': suggestion['alternative'],\n",
        "            'tracks_to_add': suggestion['tracks_lost']\n",
        "        })\n",
        "\n",
        "# Keep all others\n",
        "for pid in to_keep_ids:\n",
        "    info = playlist_info[pid]\n",
        "    reorganization_plan['keep'].append({\n",
        "        'name': info['name'],\n",
        "        'track_count': info['track_count']\n",
        "    })\n",
        "\n",
        "print(\"=\" * 80)\n",
        "print(\"ðŸ“‹ REORGANIZATION PLAN\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "print(f\"\\nðŸ—‘ï¸  DELETE ({len(reorganization_plan['delete'])} playlists):\")\n",
        "print(\"-\" * 80)\n",
        "for item in reorganization_plan['delete']:\n",
        "    print(f\"   â€¢ {item['name']}\")\n",
        "    print(f\"     â†’ {item['reason']}\")\n",
        "    print(f\"     â†’ Keep: {item['alternative']}\")\n",
        "    print()\n",
        "\n",
        "print(f\"\\nðŸ”€ MERGE ({len(reorganization_plan['merge'])} playlists):\")\n",
        "print(\"-\" * 80)\n",
        "for item in reorganization_plan['merge']:\n",
        "    print(f\"   â€¢ {item['name']}\")\n",
        "    print(f\"     â†’ {item['reason']}\")\n",
        "    if item['tracks_to_add'] > 0:\n",
        "        print(f\"     â†’ âš ï¸  Add {item['tracks_to_add']} tracks to: {item['merge_into']}\")\n",
        "    else:\n",
        "        print(f\"     â†’ âœ… {item['merge_into']}\")\n",
        "    print()\n",
        "\n",
        "print(f\"\\nâœ… KEEP ({len(reorganization_plan['keep'])} playlists):\")\n",
        "print(\"-\" * 80)\n",
        "# Sort by track count\n",
        "keep_sorted = sorted(reorganization_plan['keep'], key=lambda x: x['track_count'], reverse=True)\n",
        "for item in keep_sorted[:20]:  # Show top 20\n",
        "    print(f\"   â€¢ {item['name']} ({item['track_count']} tracks)\")\n",
        "if len(keep_sorted) > 20:\n",
        "    print(f\"   ... and {len(keep_sorted) - 20} more playlists\")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"ðŸ“Š REORGANIZATION SUMMARY\")\n",
        "print(\"=\" * 80)\n",
        "print(f\"   Current playlists: {len(playlists)}\")\n",
        "print(f\"   Delete: {len(reorganization_plan['delete'])}\")\n",
        "print(f\"   Merge: {len(reorganization_plan['merge'])}\")\n",
        "print(f\"   Keep: {len(reorganization_plan['keep'])}\")\n",
        "print(f\"   Final count: {len(reorganization_plan['keep']) + len(merge_groups)}\")\n",
        "print(f\"   Reduction: {len(reorganization_plan['delete']) + len(reorganization_plan['merge'])} playlists\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Export recommendations to CSV\n",
        "export_df = pd.DataFrame(consolidation_suggestions)\n",
        "\n",
        "# Add track counts using playlist_id\n",
        "export_df['track_count'] = export_df['playlist_id'].apply(\n",
        "    lambda pid: playlist_info.get(pid, {}).get('track_count', 0)\n",
        ")\n",
        "\n",
        "# Reorder columns\n",
        "export_df = export_df[['playlist_name', 'track_count', 'action', 'reason', 'tracks_lost', 'alternative']]\n",
        "\n",
        "# Save to CSV\n",
        "output_file = DATA_DIR / 'playlist_consolidation_recommendations.csv'\n",
        "export_df.to_csv(output_file, index=False)\n",
        "\n",
        "print(f\"âœ… Recommendations exported to: {output_file}\")\n",
        "print(f\"\\nðŸ“Š Summary:\")\n",
        "print(f\"   Total recommendations: {len(export_df)}\")\n",
        "print(f\"   Safe deletions (0 tracks lost): {len(export_df[export_df['tracks_lost'] == 0])}\")\n",
        "print(f\"   Merge recommendations: {len(export_df[export_df['action'] == 'merge'])}\")\n",
        "print(f\"\\nðŸ’¡ Next steps:\")\n",
        "print(f\"   1. Review the CSV file: {output_file}\")\n",
        "print(f\"   2. Manually verify recommendations\")\n",
        "print(f\"   3. Delete/merge playlists in Spotify\")\n",
        "print(f\"   4. Re-run sync to update your library\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## âœ… Done!\n",
        "\n",
        "**Summary:**\n",
        "- âœ… Identified redundant playlists using multiple similarity metrics\n",
        "- âœ… Found playlists safe to delete (zero track loss)\n",
        "- âœ… Suggested merge strategies for high-overlap playlists\n",
        "- âœ… Created reorganization plan\n",
        "- âœ… Exported recommendations to CSV\n",
        "\n",
        "**Next Steps:**\n",
        "1. Review the recommendations in the CSV file\n",
        "2. Manually verify each suggestion\n",
        "3. Delete/merge playlists in Spotify\n",
        "4. Re-run `01_sync_data.ipynb` to update your library\n",
        "5. Re-run this notebook to verify cleanup\n",
        "\n",
        "**Note:** Always verify recommendations manually before deleting playlists!\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
